{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  #**KNN & PCA | Assignment**"
      ],
      "metadata": {
        "id": "63TgwyON4oeQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n"
      ],
      "metadata": {
        "id": "Pga2uKjB5Maq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "ðŸŒŸ What is K-Nearest Neighbors (KNN)\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a supervised machine learning algorithm that can be used for both classification and regression tasks.\n",
        "Itâ€™s called â€œnon-parametricâ€ because it doesnâ€™t make assumptions about the underlying data distribution and is known as a lazy learner because it does not build a model during training â€” instead it stores the training data and waits until you ask it to make a prediction.\n",
        "\n",
        "In essence, KNN works like this:\n",
        "For any new data point you want to predict, KNN finds the K closest points (neighbors) from its training data and uses them to decide the prediction.\n",
        "\n",
        "ðŸ¤– How KNN Works (General Steps)\n",
        "\n",
        "\n",
        "1.   Choose K\n",
        "Decide how many neighbors (K) you want to consider â€” common values are 3, 5, or 7.\n",
        "2.   Calculate Distance\n",
        "Measure the distance between the new point and all points in the training dataset.\n",
        "Common distance metrics include\n",
        "\n",
        "\n",
        "*   Euclidean distance (most common)\n",
        "*   Manhattan distance\n",
        "*   Hamming distance (for categorical features)\n",
        "\n",
        "3.   Select K Nearest Neighbors\n",
        "\n",
        "*   Sort the distances and pick the K closest training points.\n",
        "4.  Make a Prediction\n",
        "\n",
        "*   The prediction method depends on the type of problem (classification vs regression).\n",
        "\n",
        "ðŸ§  KNN for Classification\n",
        "\n",
        "In classification problems, the goal is to assign a class label to a new data point.\n",
        "\n",
        "*   After identifying the K closest neighbors, KNN looks at their labels.\n",
        "*   It then uses majority voting to decide the class:\n",
        "\n",
        "the class that appears most frequently among the neighbors is the predicted label.\n",
        "\n",
        "Example (K=5):\n",
        "\n",
        "If among the 5 nearest neighbors:\n",
        "\n",
        "\n",
        "*   3 are labelled â€œspamâ€\n",
        "*   2 are labelled â€œnot spamâ€\n",
        "\n",
        "â†’ The new email would be predicted as spam because most neighbors voted for it.\n",
        "\n",
        "Key point: You usually choose an odd value of K (like 3, 5, 7) to reduce ties in voting.\n",
        "\n",
        "ðŸ“ˆ KNN for Regression\n",
        "\n",
        "For regression, KNN predicts a continuous value rather than a class label.\n",
        "\n",
        "*   After finding the K nearest neighbors, KNN averages their target values (the output variable) to make the prediction.\n",
        "\n",
        "Example (K=3):\n",
        "\n",
        "If the 3 nearest neighbors have these target values for house prices:\n",
        "\n",
        "*   250,000\n",
        "*   260,000\n",
        "*   270,000\n",
        "\n",
        "â†’ KNN predicts the new house price as the average:\n",
        "\n",
        "**250,000+260,000+270,000/3=260000**\n",
        "\n",
        "ðŸ”Ž Practical Notes\n",
        "\n",
        "No Actual Training Phase\n",
        "Unlike other models (like linear regression or decision trees), KNN doesnâ€™t learn coefficients or build a model during training â€” it just stores the training data for later use.\n",
        "\n",
        "Distance Matters\n",
        "Because KNN uses distances, feature scaling (like normalization or standardization) is important when features have different scales (e.g., age vs. salary).\n",
        "\n",
        "Large Datasets & High Dimensions\n",
        "KNN can become slow for large datasets, since it must compute the distance to every training point for each prediction. It also struggles in high-dimensional spaces (many features).\n",
        "\n",
        "| Aspect                 | Classification               | Regression                     |\n",
        "| ---------------------- | ---------------------------- | ------------------------------ |\n",
        "| Output Type            | Class label (categorical)    | Continuous value               |\n",
        "| How Prediction is Made | Majority vote of K neighbors | Average of K neighborsâ€™ values |\n",
        "| Suitable For           | Assigning categories         | Predicting numeric values      |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PEtGRw-X54sf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?\n"
      ],
      "metadata": {
        "id": "RBUqIje8-jDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "\n",
        "\n",
        "> It refers to the strange, unintuitive problems that occur when data has too many features (dimensions).\n",
        "As the number of dimensions increases:\n",
        "\n",
        "Data becomes sparse\n",
        "\n",
        "Distances between points become less meaningful\n",
        "\n",
        "Algorithms that rely on â€œclosenessâ€ begin struggling\n",
        "\n",
        "Computation becomes heavy\n",
        "\n",
        "\n",
        "\n",
        "ðŸ“‰ How It Affects KNN Performance\n",
        "\n",
        "KNN depends completely on distance.\n",
        "\n",
        "But in high-dimensional space:\n",
        "\n",
        "1. Neighbor distances become almost equal\n",
        "\n",
        "Think of this as KNN walking through fog. The algorithm tries to measure who is nearest, but:\n",
        "\n",
        "> The difference between the nearest and farthest neighbors becomes tiny\n",
        "\n",
        "> Everything looks equally distant\n",
        "\n",
        "> So â€œnearestâ€ loses meaninge\n",
        "\n",
        "This hurts classification and regression accuracy.\n",
        "\n",
        "> 2. Data becomes extremely sparse\n",
        "\n",
        "In high dimensions:\n",
        "\n",
        ">  You need many more data points to meaningfully cover the space\n",
        "\n",
        ">  Otherwise KNN has almost no close neighbors\n",
        "\n",
        ">  KNN begins giving random-ish predictions\n",
        "\n",
        "Itâ€™s like trying to find your friend in an infinite maze with only five people scattered inside.\n",
        "\n",
        "> 3. More features increase noise\n",
        "\n",
        ">  More dimensions often mean:\n",
        "\n",
        "> Some features donâ€™t matter\n",
        "\n",
        "> But distance calculations treat all features equally\n",
        "\n",
        "> Irrelevant features add noise\n",
        "\n",
        "> Distance becomes distorted\n",
        "\n",
        "KNN stumbles because it cannot tell which features truly matter.\n",
        "> 4. Computation time skyrockets\n",
        "Since KNN is a lazy learner:\n",
        "\n",
        ">  No training\n",
        "\n",
        ">  But heavy work at prediction time\n",
        "\n",
        ">  Distance must be calculated to all points in a huge feature space\n",
        "\n",
        "Higher dimensions multiply the cost.\n",
        "\n"
      ],
      "metadata": {
        "id": "cXUl6hBI-wq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?"
      ],
      "metadata": {
        "id": "MqrtbWq5BxGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms high-dimensional data into a smaller set of new variables called Principal Components (PCs).\n",
        "\n",
        "These components:\n",
        "\n",
        ">  Are linear combinations of the original features\n",
        "\n",
        ">  Capture maximum variance in the data\n",
        "\n",
        ">  Are uncorrelated\n",
        "\n",
        ">  Reduce dimensionality while preserving important information\n",
        "\n",
        "You can think of PCA as compressing your dataset without losing much meaning.\n",
        "\n",
        "ðŸŒŸ How PCA Works (Short Version)\n",
        "\n",
        "\n",
        "\n",
        "1  Standardize data\n",
        "\n",
        "2  Compute covariance matrix\n",
        "\n",
        "3  Find eigenvalues and eigenvectors\n",
        "\n",
        "4  Select top components with maximum variance\n",
        "\n",
        "5  Transform data into these new components\n",
        "\n",
        "PCA turns your features into new â€œdirectionsâ€ where the data varies most.\n",
        "\n",
        "ðŸŒŸ What PCA Does\n",
        "\n",
        ">  Removes noise\n",
        "\n",
        ">  Reduces dimensionality\n",
        "\n",
        ">  Helps KNN, SVM, clustering, etc.\n",
        "\n",
        ">  Visualizes high-dimensional data (PC1 vs PC2)\n",
        "\n",
        "\n",
        "ðŸŒŸ PCA vs Feature Selection\n",
        "\n",
        "| Aspect           | PCA                                       | Feature Selection                     |\n",
        "| ---------------- | ----------------------------------------- | ------------------------------------- |\n",
        "| Method           | Creates new transformed features          | Selects a subset of original features |\n",
        "| Data Meaning     | New components do not have direct meaning | Original meaning preserved            |\n",
        "| Type             | Feature extraction                        | Feature reduction                     |\n",
        "| Output           | Linear combinations                       | Original features only                |\n",
        "| Goal             | Maximize variance                         | Pick most important features          |\n",
        "| Interpretability | Harder to interpret                       | Easy to interpret                     |\n"
      ],
      "metadata": {
        "id": "XWYTdntWCDGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?"
      ],
      "metadata": {
        "id": "rQDsyAuLDTrR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "In Principal Component Analysis (PCA), eigenvalues and eigenvectors are the mathematical foundations used to decompose a dataset into its most informative components. They are derived by performing an eigendecomposition on the covariance matrix of the standardized data\n",
        "\n",
        "1. What are Eigenvectors?\n",
        ">  Definition: These are the directions (or axes) along which the data varies the most.\n",
        "\n",
        ">  * Role in PCA: Each eigenvector represents a principal component. They define the new coordinate system for the data, oriented so that the first axis follows the direction of maximum variance, the second axis follows the second-highest variance (orthogonal to the first), and so on.\n",
        "\n",
        "2. What are Eigenvalues?\n",
        "\n",
        ">  * Definition: These are the magnitudes (scalar values) that quantify the amount of variance captured along each corresponding eigenvector.\n",
        "\n",
        ">  * Role in PCA: They represent the importance or \"strength\" of each principal component. A larger eigenvalue means the corresponding eigenvector accounts for a larger portion of the dataset's total variability.\n",
        "\n",
        "3. Why are they Important?\n",
        "\n",
        "Eigenvalues and eigenvectors are critical for several reasons:\n",
        "\n",
        ">  * Dimensionality Reduction: By ranking eigenvalues from largest to smallest, you can identify which directions contain the most \"signal\" and which contain \"noise\". This allows you to discard components with small eigenvalues, reducing the number of variables while retaining most of the essential information.\n",
        "\n",
        ">  * Data Compression: They enable the projection of high-dimensional data onto a lower-dimensional subspace (e.g., 2D or 3D) for easier visualization and storage.\n",
        "\n",
        ">  * Feature Extraction: Eigenvectors are linear combinations of original features, revealing underlying patterns or \"latent variables\" that might not be obvious in the raw data.\n",
        "\n",
        "\n",
        ">  * Noise Reduction: Small eigenvalues often represent random fluctuations; removing their corresponding eigenvectors filters out noise, leading to more stable predictive models.\n",
        "\n",
        "\n",
        ">  * Information Quantification: The ratio of a single eigenvalue to the sum of all eigenvalues calculates the Percentage of Explained Variance, helping analysts decide exactly how many components to keep.\n"
      ],
      "metadata": {
        "id": "TVndti2wyur8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5: How do KNN and PCA complement each other when applied in a singlepipeline? Dataset:\n",
        "Use the Wine Dataset from sklearn.datasets.load_wine()\n"
      ],
      "metadata": {
        "id": "QzWHWHRTEGL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "In a single machine learning pipeline, Principal Component Analysis (PCA) and K-Nearest Neighbors (KNN) complement each other by balancing computational speed, noise reduction, and model accuracy. PCA acts as the preprocessing engine that optimizes the dataset specifically for the distance-based logic of KNN\n",
        "\n",
        "1. Mitigation of the \"Curse of Dimensionality\"\n",
        "\n",
        ">  * Problem: KNN calculates distances between points in high-dimensional space. As the number of features increases, the distance between any two points becomes nearly equal, making it difficult for KNN to identify meaningful neighbors.\n",
        "\n",
        ">  * Complementary Action: PCA reduces the number of dimensions while retaining the most significant variance, allowing KNN to operate in a more compact space where distances are more representative of actual data similarity.\n",
        "\n",
        "\n",
        "2. Enhanced Computational Efficiency\n",
        "\n",
        ">  * Speed: KNN is computationally expensive because it must calculate distances to every training point for each new prediction. PCA reduces the feature count, which can lead to a 60Ã— reduction in distance computation time.\n",
        "\n",
        ">  * Memory Footprint: By condensing high-dimensional data (e.g., thousands of image pixels) into a few principal components, the memory required to store the model is significantly loweredâ€”sometimes by as much as 28.6Ã—.\n",
        "\n",
        "3. Noise Reduction and Generalization\n",
        "\n",
        ">  * Filtering: High-dimensional data often contains irrelevant features or \"noise.\" PCA filters this out by discarding components with low eigenvalues (low variance).\n",
        "\n",
        ">  * Improved Accuracy: In many cases, removing noise through PCA allows KNN to focus on the essential \"signal,\" leading to higher classification accuracy than using KNN alone.\n",
        "\n",
        "4. Handling Multicollinearity\n",
        "\n",
        ">  * Uncorrelated Features: KNN can be biased if features are highly correlated because it might double-count the \"influence\" of a certain trait. PCA transforms these correlated features into a set of orthogonal (uncorrelated) principal components, ensuring that each feature used by KNN provides unique information.\n"
      ],
      "metadata": {
        "id": "QX87Zyba1hXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# 3. Create the Pipeline\n",
        "# PCA component selection: 5 components explain ~85% of variance in this dataset\n",
        "# KNN neighbors: Default is 5\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=5)),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "\n",
        "# 4. Train and Predict\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# 5. Evaluate\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olOfuTt81YmA",
        "outputId": "8b388064-8170-4541-ae28-0ffab192080e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class_0       1.00      1.00      1.00        18\n",
            "     class_1       1.00      0.95      0.98        21\n",
            "     class_2       0.94      1.00      0.97        15\n",
            "\n",
            "    accuracy                           0.98        54\n",
            "   macro avg       0.98      0.98      0.98        54\n",
            "weighted avg       0.98      0.98      0.98        54\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "xbd1WpQg2ws4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# 2. Split into training and testing sets (using a fixed seed for 2026 reproducibility)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. CASE 1: KNN Without Scaling\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "acc_unscaled = accuracy_score(y_test, knn_unscaled.predict(X_test))\n",
        "\n",
        "# 4. CASE 2: KNN With Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "acc_scaled = accuracy_score(y_test, knn_scaled.predict(X_test_scaled))\n",
        "\n",
        "# 5. Output Results\n",
        "print(f\"Accuracy WITHOUT Scaling: {acc_unscaled:.4f}\")\n",
        "print(f\"Accuracy WITH Scaling:    {acc_scaled:.4f}\")\n",
        "print(f\"Improvement:              {((acc_scaled - acc_unscaled) * 100):.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5_qUkbj1n6a",
        "outputId": "79a6e76c-c9d4-48f8-cb23-d2faeae0de50"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT Scaling: 0.7407\n",
            "Accuracy WITH Scaling:    0.9630\n",
            "Improvement:              22.22%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "rXXkrbxR3P2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# 2. Standardize the features (Crucial for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Train the PCA model\n",
        "# We set n_components=None to calculate all possible components\n",
        "pca = PCA(n_components=None)\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# 4. Access and print the explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "print(\"Explained Variance Ratio for each Principal Component:\")\n",
        "for i, ratio in enumerate(explained_variance):\n",
        "    print(f\"PC{i+1}: {ratio:.4f} ({ratio*100:.2f}%)\")\n",
        "\n",
        "# Optional: Print cumulative variance to see how much total info is retained\n",
        "cumulative_variance = explained_variance.cumsum()\n",
        "print(f\"\\nCumulative Variance (PC1 + PC2): {cumulative_variance[1]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSSCzrNl3JXw",
        "outputId": "dd641fb1-957b-4094-a8b6-c432f6b1bdcb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio for each Principal Component:\n",
            "PC1: 0.3620 (36.20%)\n",
            "PC2: 0.1921 (19.21%)\n",
            "PC3: 0.1112 (11.12%)\n",
            "PC4: 0.0707 (7.07%)\n",
            "PC5: 0.0656 (6.56%)\n",
            "PC6: 0.0494 (4.94%)\n",
            "PC7: 0.0424 (4.24%)\n",
            "PC8: 0.0268 (2.68%)\n",
            "PC9: 0.0222 (2.22%)\n",
            "PC10: 0.0193 (1.93%)\n",
            "PC11: 0.0174 (1.74%)\n",
            "PC12: 0.0130 (1.30%)\n",
            "PC13: 0.0080 (0.80%)\n",
            "\n",
            "Cumulative Variance (PC1 + PC2): 0.5541\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "H8gFwlPK3fzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load data\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# 2. Split data (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Preprocessing: Standardize both sets\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Scenario A: KNN on all 13 features\n",
        "knn_full = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_full.fit(X_train_scaled, y_train)\n",
        "acc_full = accuracy_score(y_test, knn_full.predict(X_test_scaled))\n",
        "\n",
        "# 5. Scenario B: KNN on 2 Principal Components\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "acc_pca = accuracy_score(y_test, knn_pca.predict(X_test_pca))\n",
        "\n",
        "# 6. Output Results\n",
        "print(f\"Accuracy with all 13 features:   {acc_full:.4f}\")\n",
        "print(f\"Accuracy with 2 PCA components: {acc_pca:.4f}\")\n",
        "print(f\"Variance explained by 2 PCs:    {np.sum(pca.explained_variance_ratio_):.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZNR2VIZ3pFf",
        "outputId": "4351a1d5-14b7-4b7e-c770-11fd5e76d922"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with all 13 features:   0.9630\n",
            "Accuracy with 2 PCA components: 0.9815\n",
            "Variance explained by 2 PCs:    54.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "Cov9lcdG37-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load and split the Wine dataset\n",
        "wine = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    wine.data, wine.target, test_size=0.3, random_state=42, stratify=wine.target\n",
        ")\n",
        "\n",
        "# 2. Scale the features (Essential for distance-based metrics)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 3. Define metrics to compare\n",
        "metrics = ['euclidean', 'manhattan']\n",
        "results = {}\n",
        "\n",
        "for metric in metrics:\n",
        "    # Initialize KNN with specific metric\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predict and calculate accuracy\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results[metric] = acc\n",
        "\n",
        "# 4. Output Comparison\n",
        "print(f\"{'Metric':<12} | {'Accuracy':<10}\")\n",
        "print(\"-\" * 25)\n",
        "for metric, acc in results.items():\n",
        "    print(f\"{metric.capitalize():<12} | {acc:.4%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUijqGpA32rl",
        "outputId": "d76d3e4b-0e92-44d2-e5c9-cfb2c5832390"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metric       | Accuracy  \n",
            "-------------------------\n",
            "Euclidean    | 94.4444%\n",
            "Manhattan    | 98.1481%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: You are working with a high-dimensional gene expression dataset to lassify patients with different types of cancer. Due to the large number of features and a small number of samples, traditional models overfit.\n",
        "\n",
        "Explain how you would:\n",
        "\n",
        "â— Use PCA to reduce dimensionality\n",
        "\n",
        "â— Decide how many components to keep\n",
        "\n",
        "â— Use KNN for classification post-dimensionality reduction\n",
        "\n",
        "â— Evaluate the model\n",
        "\n",
        "â— Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "Hgk2ReWK48bH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "1. PCA Dimensionality Reduction: We use PCA to transform thousands of gene\n",
        "expression levels into a small set of uncorrelated principal components. This filters out biological and technical noise while retaining the strongest diagnostic signals.\n",
        "\n",
        "2. Determining Components: We use the Cumulative Explained Variance method. In biomedical contexts, we typically retain enough components to explain 90â€“95% of the total variance, or use the \"Elbow Method\" on a Scree plot.\n",
        "\n",
        "3. KNN Classification: After reduction, KNN classifies patients by comparing their component \"fingerprint\" to known cases. In lower-dimensional PCA space, the distance metrics become statistically significant rather than being diluted by noise.\n",
        "\n",
        "4. Evaluation: Given the small sample size, we use Stratified K-Fold Cross-Validation to ensure the model generalizes across different patient subgroups and avoids \"lucky\" splits.\n",
        "\n",
        "5. Stakeholder Justification:\n",
        "\n",
        "\n",
        "*   Robustness: PCA prevents the model from \"memorizing\" noise in specific genes (overfitting).\n",
        "\n",
        "*   Efficiency: Smaller data footprints allow for faster diagnostic tools.\n",
        "\n",
        "*   Interpretation: While raw PCA is abstract, we can map components back to original genes to identify key biological biomarkers.\n"
      ],
      "metadata": {
        "id": "6-Z3Feoa5gT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# 1. Simulate High-Dimensional Gene Data (100 patients, 5000 genes)\n",
        "X, y = make_classification(n_samples=100, n_features=5000, n_informative=50, n_classes=2, random_state=42)\n",
        "\n",
        "# 2. Construct the Pipeline\n",
        "# We choose n_components=0.95 to keep enough PCs to explain 95% of variance\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=0.95)),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=3, weights='distance'))\n",
        "])\n",
        "\n",
        "# 3. Evaluate using Stratified Cross-Validation (Robust for small samples)\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')\n",
        "\n",
        "# 4. Train to find how many components were actually kept\n",
        "pipeline.fit(X, y)\n",
        "n_pcs = pipeline.named_steps['pca'].n_components_\n",
        "\n",
        "print(f\"Original Feature Count: 5000\")\n",
        "print(f\"Principal Components Retained (95% Variance): {n_pcs}\")\n",
        "print(f\"Mean CV Accuracy: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hch92KGU4EC-",
        "outputId": "51f03723-7158-4df6-fc1f-afceaa4ba339"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Feature Count: 5000\n",
            "Principal Components Retained (95% Variance): 93\n",
            "Mean CV Accuracy: 0.4700 (+/- 0.1208)\n"
          ]
        }
      ]
    }
  ]
}