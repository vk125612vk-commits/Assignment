{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 :  What is the difference between AI, ML, DL, and Data Science? Provide a\n",
        "brief explanation of each.\n",
        "(Hint: Compare their scope, techniques, and applications for each.)"
      ],
      "metadata": {
        "id": "nRi6amrZn1Mg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - 1. **Artificial Intelligence (AI)**\n",
        "\n",
        "Scope:\n",
        "AI is the broadest field. It focuses on building machines that can perform tasks requiring human intelligence.\n",
        "\n",
        "Techniques:\n",
        "\n",
        "Rule-based systems\n",
        "\n",
        "Search algorithms\n",
        "\n",
        "Knowledge graphs\n",
        "\n",
        "Machine learning\n",
        "\n",
        "Natural language processing (NLP)\n",
        "\n",
        "Robotics\n",
        "\n",
        "Applications:\n",
        "\n",
        "Chatbots\n",
        "\n",
        "Self-driving cars\n",
        "\n",
        "Expert systems\n",
        "\n",
        "Speech assistants (Alexa, Siri)\n",
        "\n",
        " - 2. **Machine Learning (ML)**\n",
        "\n",
        "Scope:\n",
        "ML is a subset of AI. It enables machines to learn patterns from data without being explicitly programmed.\n",
        "\n",
        "Techniques:\n",
        "\n",
        "Supervised learning (Regression, Classification)\n",
        "\n",
        "Unsupervised learning (Clustering, PCA)\n",
        "\n",
        "Reinforcement learning\n",
        "\n",
        "Applications:\n",
        "\n",
        "Spam detection\n",
        "\n",
        "Recommendation systems\n",
        "\n",
        "Fraud detection\n",
        "\n",
        "Customer segmentation\n",
        "\n",
        " - 3. **Deep Learning (DL)**\n",
        "\n",
        "Scope:\n",
        "DL is a subset of ML. It uses artificial neural networks with many layers to learn complex patterns.\n",
        "\n",
        "Techniques:\n",
        "\n",
        "Convolutional Neural Networks (CNNs)\n",
        "\n",
        "Recurrent Neural Networks (RNNs)\n",
        "\n",
        "Transformers\n",
        "\n",
        "Autoencoders\n",
        "\n",
        "Applications:\n",
        "\n",
        "Image recognition\n",
        "\n",
        "Speech recognition\n",
        "\n",
        "Self-driving cars (vision)\n",
        "\n",
        "Face detection\n",
        "\n",
        " - 4. **Data Science**\n",
        "\n",
        "Scope:\n",
        "Data Science is broader than ML. It focuses on extracting insights from data using mathematical, statistical, and engineering techniques.\n",
        "\n",
        "Techniques:\n",
        "\n",
        "Statistics & probability\n",
        "\n",
        "Data cleaning & preprocessing\n",
        "\n",
        "Data visualization\n",
        "\n",
        "Machine learning modeling\n",
        "\n",
        "Big data tools (Hadoop, Spark)\n",
        "\n",
        "Applications:\n",
        "\n",
        "Business analytics\n",
        "\n",
        "Forecasting\n",
        "\n",
        "Market research\n",
        "\n",
        "KPI reporting\n",
        "\n",
        "Decision-making"
      ],
      "metadata": {
        "id": "cQoXawaIo3SH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Aspect               | AI                                    | ML                          | DL                                        | Data Science                   |\n",
        "| -------------------- | ------------------------------------- | --------------------------- | ----------------------------------------- | ------------------------------ |\n",
        "| **Definition**       | Machines mimicking human intelligence | Machines learning from data | Neural networks learning complex patterns | Extracting knowledge from data |\n",
        "| **Subset of**        | ‚Äî                                     | AI                          | ML                                        | ‚Äî                              |\n",
        "| **Techniques**       | Rules, ML, NLP, Robotics              | Regression, Classification  | CNN, RNN, Transformers                    | Stats + ML + Visualization     |\n",
        "| **Focus**            | Automation & intelligence             | Pattern learning            | High-level pattern recognition            | Insights and business value    |\n",
        "| **Data Requirement** | Medium                                | High                        | Very high (large datasets)                | Varies                         |\n"
      ],
      "metadata": {
        "id": "pmQ1yI0cohpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain overfitting and underfitting in ML. How can you detect and prevent\n",
        "them?\n",
        "Hint: Discuss bias-variance tradeoff, cross-validation, and regularization techniques."
      ],
      "metadata": {
        "id": "7MiWYR22n5kn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - 1.  **Overfitting\n",
        "Definition**:\n",
        "\n",
        "Overfitting occurs when a model learns the training data too well, including noise and irrelevant patterns.\n",
        "It performs excellent on training data but poor on test/unseen data.\n",
        "\n",
        "**Characteristics:**\n",
        "\n",
        "Very low training error\n",
        "\n",
        "High test/validation error\n",
        "\n",
        "Model is too complex (high variance)\n",
        "\n",
        "**How to Detect Overfitting:**\n",
        "\n",
        "Large gap between training accuracy and validation accuracy\n",
        "\n",
        "Learning curves diverge (train ‚Üë, test ‚Üì)\n",
        "\n",
        "**How to Prevent Overfitting:**\n",
        "\n",
        "Cross-Validation\n",
        "\n",
        "Use k-fold CV to evaluate performance on multiple subsets.\n",
        "\n",
        "Regularization\n",
        "\n",
        "L1 (Lasso) and L2 (Ridge) add penalties to reduce complexity.\n",
        "\n",
        "Simplify the Model\n",
        "\n",
        "Reduce depth of trees, fewer layers in neural networks.\n",
        "\n",
        "Early Stopping\n",
        "\n",
        "Stop training when validation loss starts increasing.\n",
        "\n",
        "Dropout (DL)\n",
        "\n",
        "Randomly drop neurons to avoid co-dependency.\n",
        "\n",
        "More Data / Data Augmentation\n",
        "\n",
        "Helps the model generalize better.\n",
        "\n",
        " - 2. **Underfitting**\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "Underfitting happens when the model is too simple to learn the underlying pattern.\n",
        "It performs poorly on both training and test data.\n",
        "\n",
        "**Characteristics:**\n",
        "\n",
        "High training error\n",
        "\n",
        "High test error\n",
        "\n",
        "Model has high bias, low variance\n",
        "\n",
        "**How to Detect Underfitting:**\n",
        "\n",
        "Training accuracy is low\n",
        "\n",
        "Both training & validation curves are close but low\n",
        "\n",
        "Model not capturing structure of data\n",
        "\n",
        "**How to Prevent Underfitting:**\n",
        "\n",
        "Increase Model Complexity\n",
        "\n",
        "Use deeper decision trees or more features.\n",
        "\n",
        "Reduce Regularization\n",
        "\n",
        "Lower lambda (Œ±) in L1/L2.\n",
        "\n",
        "Train Longer\n",
        "\n",
        "More epochs/iterations.\n",
        "\n",
        "Feature Engineering\n",
        "\n",
        "Add new meaningful variables."
      ],
      "metadata": {
        "id": "9ndmgbj4pUXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:How would you handle missing values in a dataset? Explain at least three\n",
        "methods with examples.\n",
        "Hint: Consider deletion, mean/median imputation, and predictive modeling."
      ],
      "metadata": {
        "id": "D_Y6OWSrn7Qo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing values are common in real-world datasets and can affect model performance. Here are three effective methods with explanations and examples.\n",
        "\n",
        " - **1. Deletion Methods (Removing Data)**\n",
        "\n",
        "a) Listwise Deletion (Remove entire rows)\n",
        "\n",
        "Remove rows where any column has a missing value.\n",
        "Works when:\n",
        "\n",
        "‚úî Missing values are very few\n",
        "\n",
        "‚úî Data is missing completely at random (MCAR)\n",
        "\n",
        "| Age | Salary |\n",
        "| --- | ------ |\n",
        "| 25  | 40,000 |\n",
        "| NaN | 50,000 |\n",
        "| 30  | NaN    |\n",
        "**After listwise deletion:**\n",
        "| Age | Salary |\n",
        "| --- | ------ |\n",
        "| 25  | 40,000 |\n",
        "\n",
        " - **b) Column Deletion (Remove entire column)**\n",
        "\n",
        "Useful when:\n",
        "\n",
        "‚úî One feature has more than 60‚Äì70% missing values\n",
        "\n",
        "‚úî Feature is not important\n",
        "\n",
        " - **2. Imputation Methods**\n",
        "\n",
        "**a) Mean/Median/Mode Imputation**\n",
        "\n",
        "\n",
        "Replace missing values with a statistical measure.\n",
        "\n",
        "**Mean Imputation (for numerical data)**\n",
        "\n",
        "Example:\n",
        "Column: [10, 20, NaN, 30]\n",
        "Mean = 20\n",
        "Imputed column: [10, 20, 20, 30]\n",
        "\n",
        "**Median Imputation (for skewed data)**\n",
        "\n",
        "Good for income, house prices, etc.\n",
        "\n",
        "Example:\n",
        "Column: [5, 7, NaN, 100]\n",
        "Median = 7\n",
        "Imputed column: [5, 7, 7, 100]\n",
        "\n",
        "**Mode Imputation (for categorical data)**\n",
        "\n",
        "Example:\n",
        "Column: ['Red', NaN, 'Blue', 'Red']\n",
        "Mode = \"Red\"\n",
        "Imputed column: ['Red', 'Red', 'Blue', 'Red']\n",
        "\n",
        "- **3. Predictive Modeling (Advanced Method)**\n",
        "\n",
        "Use ML models to predict missing values based on other features.\n",
        "\n",
        "**a) Regression Imputation**\n",
        "\n",
        "Predict numerical missing values.\n",
        "\n",
        "Example:\n",
        "Missing Salary ‚Üí Use Age, Experience to predict Salary using Linear Regression.\n",
        "\n",
        "**b) Classification Imputation**\n",
        "\n",
        "Predict categorical values.\n",
        "\n",
        "Example:\n",
        "Missing Gender ‚Üí Use Decision Tree Classifier.\n",
        "\n",
        "**c) KNN Imputation**\n",
        "\n",
        "Finds K nearest neighbors\n",
        "\n",
        "Fills missing values with average (numerical) or most common (categorical)\n",
        "\n",
        "Example:\n",
        "If K = 3 and neighbors‚Äô values are 40k, 42k, 38k ‚Üí Imputed = 40k\n",
        "\n",
        "Libraries: sklearn.impute.KNNImputer\n",
        "\n",
        " - **4. Other Methods (Optional for longer answers)**\n",
        "\n",
        "Forward/Backward Fill (for time series)\n",
        "\n",
        "Interpolation (fills trends)\n",
        "\n",
        "Multiple Imputation (MICE)"
      ],
      "metadata": {
        "id": "YpbFyxzIqXFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4:What is an imbalanced dataset? Describe two techniques to handle it\n",
        "(theoretical + practical).\n",
        "Hint: Discuss SMOTE, Random Under/Oversampling, and class weights in models."
      ],
      "metadata": {
        "id": "PXrThl7Hn88I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - An imbalanced dataset is a dataset where the classes (labels) are not represented equally.\n",
        "One class has many more samples than the other.\n",
        "\n",
        "Example:\n",
        "\n",
        "Fraud detection dataset:\n",
        "\n",
        "Non-fraud: 98%\n",
        "\n",
        "Fraud: 2%\n",
        "\n",
        "The model becomes biased toward the majority class and performs poorly on the minority class.\n",
        "\n",
        "**Techniques to Handle Imbalanced Data**\n",
        "\n",
        " - Below are two widely used techniques, each with theory + practical examples.\n",
        " Techniques to Handle Imbalanced Data\n",
        "\n",
        "Below are two widely used techniques, each with theory + practical examples.\n",
        "\n",
        "**Technique 1: Random Under/Oversampling**\n",
        "\n",
        " **A. Random Oversampling (Increase minority samples)**\n",
        "Theory:\n",
        "\n",
        "Duplicate or randomly generate additional samples of the minority class.\n",
        "\n",
        "Balances the dataset by increasing minority instances.\n",
        "\n",
        "Practical Example:\n",
        "\n",
        "Before:\n",
        "\n",
        "Class 0 = 900\n",
        "\n",
        "Class 1 = 100\n",
        "\n",
        "After oversampling:\n",
        "\n",
        "Class 0 = 900\n",
        "\n",
        "Class 1 = 900 (minority duplicated)\n",
        "\n",
        "\n",
        " - **B. Random Undersampling (Reduce majority samples)**\n",
        "\n",
        "**Theory:**\n",
        "\n",
        "Remove samples from the majority class to balance classes.\n",
        "\n",
        "Faster but may lose information.\n",
        "\n",
        "**Practical Example:**\n",
        "\n",
        "Before:\n",
        "\n",
        "Class 0 = 900\n",
        "\n",
        "Class 1 = 100\n",
        "\n",
        "After undersampling:\n",
        "\n",
        "Class 0 = 100\n",
        "\n",
        "Class 1 = 100\n",
        "\n",
        " - **Technique 2: SMOTE (Synthetic Minority Oversampling Technique)**\n",
        "\n",
        "**Theory:**\n",
        "\n",
        "Instead of duplicating existing samples, SMOTE generates new synthetic samples for the minority class.\n",
        "\n",
        "Uses k-nearest neighbors to create similar new points.\n",
        "\n",
        "Prevents overfitting that happens with simple oversampling.\n",
        "\n",
        "**Practical Example:**\n",
        "\n",
        "If minority sample is:\n",
        "(Age=30, Salary=50k)\n",
        "SMOTE generates new synthetic points like:\n",
        "(Age=31, Salary=49k), (Age=29, Salary=52k) etc.\n",
        "\n",
        " - **Technique 3 (Bonus): Class Weights in ML Models**\n",
        "\n",
        "**Theory:**\n",
        "\n",
        "Assign higher weight to minority class during training.\n",
        "This makes the model penalty higher for misclassifying the minority class.\n",
        "\n",
        "**Works well with:**\n",
        "\n",
        "Logistic Regression\n",
        "\n",
        "SVM\n",
        "\n",
        "Random Forest\n",
        "\n",
        "XGBoost"
      ],
      "metadata": {
        "id": "5Slg_J8rrqHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Why is feature scaling important in ML? Compare Min-Max scaling and\n",
        "Standardization.\n",
        "Hint: Explain impact on distance-based algorithms (e.g., KNN, SVM) and gradient\n",
        "descent."
      ],
      "metadata": {
        "id": "rwrfra9on-pX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Feature scaling is the process of transforming numerical features so they have a similar range.\n",
        "It is important because many ML algorithms depend on distance, gradient updates, or magnitude of features.\n",
        "\n",
        "**Why it matters:**\n",
        "**1. Distance-based algorithms**\n",
        "\n",
        "Algorithms like KNN, K-means, SVM compute distances between points.\n",
        "\n",
        "Without scaling:\n",
        "\n",
        "Features with large ranges dominate distance calculations.\n",
        "\n",
        "Example:\n",
        "Age range: 0‚Äì70  \n",
        "Salary range: 0‚Äì200000  \n",
        "‚Üí Salary dominates distance\n",
        "\n",
        " - **2. Gradient Descent‚Äìbased models**\n",
        "\n",
        "Linear Regression, Logistic Regression, Neural Networks\n",
        "\n",
        "If features have large differences in scale:\n",
        "\n",
        "Gradient descent becomes slow\n",
        "\n",
        "Convergence takes longer\n",
        "\n",
        "Loss surface becomes sloppy\n",
        "\n",
        " - **3. Regularization Effects**\n",
        "\n",
        "L1/L2 penalize large weights\n",
        "\n",
        "Without scaling:\n",
        "\n",
        "Features with larger ranges get higher penalties\n",
        "\n",
        "Leads to biased models\n",
        "\n",
        " - **1. Min-Max Scaling (Normalization)**\n",
        "Formula:\n",
        "ùë•‚Ä≤=ùë•‚àíùë•ùëöùëñùëõùë•ùëöùëéùë•‚àíùë•ùëöùëñùëõx‚Ä≤=xmax‚àíxminx‚àíxmin\n",
        "\t‚Äã\n",
        "**Range:**\n",
        "\n",
        "‚Üí Transforms values to 0 to 1\n",
        "\n",
        "**Best for:**\n",
        "\n",
        "Neural Networks\n",
        "\n",
        "KNN\n",
        "\n",
        "K-means\n",
        "\n",
        "Example:\n",
        "\n",
        "Original values: [10, 20, 30]\n",
        "Min = 10, Max = 30\n",
        "Scaled values:\n",
        "[(10-10)/20 = 0, (20-10)/20 = 0.5, (30-10)/20 = 1]\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "Keeps original shape of the distribution\n",
        "\n",
        "Useful when data has bounded values\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "Sensitive to outliers\n",
        "\n",
        "A single extreme value affects entire scaling\n",
        "\n",
        " - **2. Standardization (Z-score Scaling)**\n",
        "\n",
        "Formula:ùë•‚Ä≤=ùë•‚àíùúáùúéx‚Ä≤=œÉx‚àíŒº\n",
        "\t‚Äã\n",
        "**Range:**\n",
        "\n",
        "‚Üí Mean = 0, Standard Deviation = 1\n",
        "(Not bound to 0‚Äì1)\n",
        "\n",
        "**Best for:**\n",
        "\n",
        "Linear Regression\n",
        "\n",
        "Logistic Regression\n",
        "\n",
        "SVM\n",
        "\n",
        "PCA\n",
        "\n",
        "Gradient descent‚Äìbased models\n",
        "\n",
        "Example:\n",
        "\n",
        "Values: [10, 20, 30]\n",
        "Mean = 20, Std = 8.16\n",
        "Scaled: [ -1.22, 0, 1.22 ]\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "Works well with outliers\n",
        "\n",
        "Essential for algorithms assuming normal distribution (PCA)\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "Does not bound values to a specific range\n",
        "\n",
        "Still influenced by extreme outliers\n",
        "\n",
        "| Feature               | Min-Max Scaling       | Standardization   |\n",
        "| --------------------- | --------------------- | ----------------- |\n",
        "| Output Range          | 0 to 1                | Mean = 0, Std = 1 |\n",
        "| Sensitive to Outliers | Yes                   | Less              |\n",
        "| Best For              | KNN, NN, K-means      | SVM, LR, PCA      |\n",
        "| Distribution          | Preserved             | Centered + scaled |\n",
        "| Formula               | (x - min)/(max - min) | (x - Œº)/œÉ         |\n"
      ],
      "metadata": {
        "id": "1eOJz3L8tYOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6:  Compare Label Encoding and One-Hot Encoding. When would you prefer\n",
        "one over the other?\n",
        "Hint: Consider categorical variables with ordinal vs. nominal relationships."
      ],
      "metadata": {
        "id": "v7idABACoA3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - **1. Label Encoding**\n",
        "\n",
        "**What it does:**\n",
        "\n",
        "Converts categories into integer labels (0, 1, 2, 3 ‚Ä¶).\n",
        "\n",
        "Example:\n",
        "\n",
        "Size = [\"Small\", \"Medium\", \"Large\"]\n",
        "\n",
        "Label Encoded:\n",
        "\n",
        "Small ‚Üí 0\n",
        "\n",
        "Medium ‚Üí 1\n",
        "\n",
        "Large ‚Üí 2\n",
        "\n",
        "\n",
        "**When to Use:**\n",
        "\n",
        "When the categorical variable is ordinal (i.e., has natural order).\n",
        "\n",
        "Examples:\n",
        "\n",
        "Size (Small < Medium < Large)\n",
        "\n",
        "Education (High School < Bachelor < Master < PhD)\n",
        "\n",
        "Customer rating (Low < Medium < High)\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "Simple and memory-efficient\n",
        "\n",
        "Preserves order\n",
        "\n",
        "Works well with Tree-based models (Decision Trees, Random Forest, XGBoost)\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "Implies ordering even when none exists\n",
        "\n",
        "Can mislead algorithms like KNN, SVM, Linear Regression\n",
        "(because they assume numerical meaning)\n",
        "\n",
        " - **2. One-Hot Encoding**\n",
        "\n",
        "**What it does:**\n",
        "\n",
        "Creates a new binary column (0/1) for each category.\n",
        "\n",
        "Example:\n",
        "\n",
        "Color = [\"Red\", \"Blue\", \"Green\"]\n",
        "\n",
        "\n",
        "One-Hot Encoded:\n",
        "\n",
        "Red   ‚Üí [1,0,0]\n",
        "\n",
        "Blue  ‚Üí [0,1,0]\n",
        "\n",
        "Green ‚Üí [0,0,1]\n",
        "\n",
        "\n",
        "**When to Use:**\n",
        "\n",
        "When the categorical variable is nominal (no ordering).\n",
        "\n",
        "Examples:\n",
        "\n",
        "Colors (Red, Blue, Green)\n",
        "\n",
        "Cities (Delhi, Mumbai, Kolkata)\n",
        "\n",
        "Gender (Male, Female)\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "No false ordinal relationships\n",
        "\n",
        "Works well with distance-based models & linear models\n",
        "(KNN, SVM, Logistic Regression, Neural Networks)\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "Creates many columns ‚Üí high dimensionality\n",
        "\n",
        "Inefficient for variables with large categories (e.g., 1000+ unique values)\n"
      ],
      "metadata": {
        "id": "97cqCPnBuw5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7:  Google Play Store Dataset\n",
        "a). Analyze the relationship between app categories and ratings. Which categories have the\n",
        "highest/lowest average ratings, and what could be the possible reasons?\n",
        "Dataset: https://github.com/MasteriNeuron/datasets.git\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "gryqzfBXoEAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# load dataset (adjust path as needed)\n",
        "df = pd.read_csv('googleplaystore.csv')\n",
        "\n",
        "# quick look\n",
        "print(\"Initial shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Clean data: handle invalid / missing ratings\n",
        "# Some rows may have Rating = NaN ‚Äî drop them for simplicity\n",
        "df_clean = df.dropna(subset=['Rating'])\n",
        "\n",
        "# Also convert Rating to numeric (should be already), but ensure\n",
        "df_clean['Rating'] = pd.to_numeric(df_clean['Rating'], errors='coerce')\n",
        "df_clean = df_clean.dropna(subset=['Rating'])\n",
        "\n",
        "# Now compute average rating by category\n",
        "mean_rating_by_cat = df_clean.groupby('Category')['Rating'].mean().sort_values(ascending=False)\n",
        "\n",
        "print(\"\\nAverage rating by category:\")\n",
        "print(mean_rating_by_cat)\n",
        "\n",
        "# Also view count per category (to check if some categories have very few apps)\n",
        "count_by_cat = df_clean['Category'].value_counts()\n",
        "\n",
        "print(\"\\nNumber of apps per category:\")\n",
        "print(count_by_cat)\n",
        "\n",
        "# Combine into one DataFrame for clarity\n",
        "summary = pd.DataFrame({\n",
        "    'count': count_by_cat,\n",
        "    'mean_rating': mean_rating_by_cat\n",
        "}).sort_values(by='mean_rating', ascending=False)\n",
        "\n",
        "print(\"\\nSummary (count + mean_rating):\")\n",
        "print(summary)\n",
        "\n",
        "# Optionally: For categories with very few apps, you might want to ignore them\n",
        "# For example: only consider categories with count >= 50\n",
        "summary_filtered = summary[ summary['count'] >= 50 ]\n",
        "print(\"\\nFiltered (categories with >= 50 apps):\")\n",
        "print(summary_filtered)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "oQhmcrLVvshZ",
        "outputId": "cf68d2f7-56da-40e7-a89b-a16e8eb50380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'googleplaystore.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2263240424.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# load dataset (adjust path as needed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'googleplaystore.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# quick look\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'googleplaystore.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Titanic Dataset\n",
        "a) Compare the survival rates based on passenger class (Pclass). Which class had the highest\n",
        "survival rate, and why do you think that happened?\n",
        "b) Analyze how age (Age) affected survival. Group passengers into children (Age < 18) and\n",
        "adults (Age ‚â• 18). Did children have a better chance of survival?\n",
        "Dataset: https://github.com/MasteriNeuron/datasets.git\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "H1PkKES8oH04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load Titanic dataset\n",
        "df = pd.read_csv(\"titanic.csv\")   # Change path if needed\n",
        "\n",
        "# Display first few rows\n",
        "print(df.head())\n",
        "\n",
        "# ---------------------------- PART A ----------------------------\n",
        "# Survival rate by Passenger Class (Pclass)\n",
        "\n",
        "survival_by_class = df.groupby(\"Pclass\")[\"Survived\"].mean().sort_values(ascending=False)\n",
        "print(\"\\nSurvival Rate by Passenger Class:\")\n",
        "print(survival_by_class)\n",
        "\n",
        "# ---------------------------- PART B ----------------------------\n",
        "# Create Age groups: Child (<18), Adult (>=18)\n",
        "\n",
        "df['AgeGroup'] = df['Age'].apply(lambda x: \"Child\" if x < 18 else \"Adult\")\n",
        "\n",
        "# Drop missing ages\n",
        "df_age = df.dropna(subset=[\"Age\"])\n",
        "\n",
        "# Survival rate by AgeGroup\n",
        "survival_by_age = df_age.groupby(\"AgeGroup\")[\"Survived\"].mean()\n",
        "print(\"\\nSurvival Rate by Age Group (Child vs Adult):\")\n",
        "print(survival_by_age)\n",
        "\n",
        "# Also count how many in each group\n",
        "count_age_group = df_age[\"AgeGroup\"].value_counts()\n",
        "print(\"\\nCount in each Age Group:\")\n",
        "print(count_age_group)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "izXPU0aDxEE4",
        "outputId": "0ac10034-0845-4e45-b1d5-fc280a51143c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'titanic.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2931216367.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load Titanic dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"titanic.csv\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Change path if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Display first few rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanic.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Flight Price Prediction Dataset\n",
        "a) How do flight prices vary with the days left until departure? Identify any exponential price\n",
        "surges and recommend the best booking window.\n",
        "b)Compare prices across airlines for the same route (e.g., Delhi-Mumbai). Which airlines are\n",
        "consistently cheaper/premium, and why?\n",
        "Dataset: https://github.com/MasteriNeuron/datasets.git\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "ibcpnv1ooJvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset (adjust path as needed)\n",
        "df = pd.read_csv(\"flight_price.csv\")     # file from MasteriNeuron GitHub dataset\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "# -------------------------------- PART A --------------------------------\n",
        "# Price vs Days Left\n",
        "\n",
        "# Clean numeric fields\n",
        "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
        "df['days_left'] = pd.to_numeric(df['days_left'], errors='coerce')\n",
        "df = df.dropna(subset=['price', 'days_left'])\n",
        "\n",
        "# Group by number of days left & calculate mean price\n",
        "price_by_days = df.groupby('days_left')['price'].mean().reset_index()\n",
        "\n",
        "print(\"\\nAverage Price by Days Left:\")\n",
        "print(price_by_days)\n",
        "\n",
        "# -------------------------------- PART B --------------------------------\n",
        "# Airline Price Comparison for a Route (Delhi ‚Üí Mumbai)\n",
        "\n",
        "route_df = df[(df['source_city'] == \"Delhi\") & (df['destination_city'] == \"Mumbai\")]\n",
        "\n",
        "airline_price = route_df.groupby('airline')['price'].mean().sort_values()\n",
        "\n",
        "print(\"\\nAverage Airline Prices for Delhi ‚Üí Mumbai:\")\n",
        "print(airline_price)\n",
        "\n",
        "# Count number of flights per airline (to ensure reliability)\n",
        "airline_count = route_df['airline'].value_counts()\n",
        "print(\"\\nNumber of Flights per Airline:\")\n",
        "print(airline_count)\n"
      ],
      "metadata": {
        "id": "ryTI2N72xTlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10:  HR Analytics Dataset\n",
        "a). What factors most strongly correlate with employee attrition? Use visualizations to show key\n",
        "drivers (e.g., satisfaction, overtime, salary).\n",
        "b). Are employees with more projects more likely to leave?  \n",
        "Dataset: [hr_analytics](https://github.com/MasteriNeuron/datasets/blob/e0da5fc18ebf4e3dc90950b932d6bded61e0ee49/hr_analytics.csv)\n"
      ],
      "metadata": {
        "id": "Z-gZivaloKTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"hr_analytics.csv\")\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "# -------------------------------- PART A --------------------------------\n",
        "# Correlation with Attrition\n",
        "\n",
        "# Convert Attrition 'Yes'/'No' ‚Üí 1/0\n",
        "df['Attrition'] = df['Attrition'].map({\"Yes\": 1, \"No\": 0})\n",
        "\n",
        "# Numeric columns for correlation\n",
        "numeric_cols = ['Attrition', 'MonthlyIncome', 'Age', 'NumCompaniesWorked',\n",
        "                'DistanceFromHome', 'PercentSalaryHike', 'TotalWorkingYears',\n",
        "                'YearsAtCompany', 'YearsSinceLastPromotion', 'EnvironmentSatisfaction',\n",
        "                'JobSatisfaction', 'WorkLifeBalance', 'OverTime', 'YearsInCurrentRole']\n",
        "\n",
        "# Convert OverTime to 0/1\n",
        "df['OverTime'] = df['OverTime'].map({\"Yes\": 1, \"No\": 0})\n",
        "\n",
        "corr = df[numeric_cols].corr()['Attrition'].sort_values(ascending=False)\n",
        "print(\"\\nCorrelation with Attrition:\")\n",
        "print(corr)\n",
        "\n",
        "# Heatmap (Visualization)\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(df[numeric_cols].corr(), annot=False, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Bar plot of top drivers\n",
        "top_corr = corr[1:8]     # exclude attrition=1\n",
        "top_corr.plot(kind='bar', figsize=(10,5), title=\"Top Correlated Factors with Attrition\")\n",
        "plt.ylabel(\"Correlation\")\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------- PART B --------------------------------\n",
        "# Effect of Number of Projects on Attrition\n",
        "\n",
        "projects_attrition = df.groupby('NumCompaniesWorked')['Attrition'].mean()\n",
        "\n",
        "print(\"\\nAttrition Rate by Number of Companies Previously Worked:\")\n",
        "print(projects_attrition)\n",
        "\n",
        "projects_attrition.plot(kind='bar', figsize=(10,5), title=\"Attrition vs Number of Companies Worked\")\n",
        "plt.ylabel(\"Attrition Rate (0 to 1)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "S5eHyL_Fxdvw",
        "outputId": "020958aa-c999-47f0-bd36-8049d0a53d66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'hr_analytics.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2800128617.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hr_analytics.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hr_analytics.csv'"
          ]
        }
      ]
    }
  ]
}