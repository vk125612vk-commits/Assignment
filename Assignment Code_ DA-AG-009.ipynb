{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqytgFyC8jPSBQPPfuy0sj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Question 1: What is Information Gain, and how is it used in Decision Trees?**"],"metadata":{"id":"EniGG5VobVat"}},{"cell_type":"markdown","source":["Answer:\n","\n","Information Gain is a metric used to decide which feature should be used to split the data at each node of a Decision Tree. It measures the reduction in uncertainty (entropy) after a dataset is split on a particular feature.\n","\n","Higher Information Gain means better feature for splitting.\n","\n","Decision Trees choose the feature with maximum Information Gain.\n","\n","Formula:\n","Information Gain = Entropy(parent) − Weighted Entropy(children)"],"metadata":{"id":"YoCh1xdobdm6"}},{"cell_type":"markdown","source":["**Question 2: What is the difference between Gini Impurity and Entropy?\n","Hint: Directly compares the two main impurity measures, highlighting strengths,\n","weaknesses, and appropriate use cases**"],"metadata":{"id":"NQdh_5ypboQ9"}},{"cell_type":"markdown","source":["Answer\n","\n","\n","\n","Gini Impurity and Entropy are both measures of impurity/uncertainty used in decision tree algorithms, with the primary difference being computational efficiency. Gini Impurity is faster to compute, while Entropy is more theoretically grounded in information theory and can produce slightly better, more balanced splits in some cases\n","formula:\n","| Situation                                                | Better Choice                                      |\n","| -------------------------------------------------------- | -------------------------------------------------- |\n","| You want **faster computation**                          | **Gini Impurity**                                  |\n","| You care about **information-theoretic interpretation**  | **Entropy**                                        |\n","| You’re mainly focused on **classification tree quality** | Either — test both with cross-validation           |\n","| You expect **imbalanced classes**                        | Entropy sometimes helps but results can be similar |\n","\n","\n","**Gini Impurity**\n","\n","**Strengths, Weaknesses, and Use Cases**\n","\n","**Strengths**\n","\n",">Speed Its main advantage is computational efficiency, making it the default choice in many real-world systems like the CART algorithm.\n","\n",">Robustness: It tends to be more robust to noise and can have lower variance.\n","\n","**Weaknesses:**\n","\n",">May perform less well when class distributions are highly imbalanced, as it favors isolating the majority class quickly.\n","\n","**Appropriate Use Cases**\n","\n",">Large datasets where training time is a primary constraint.\n",">Real-time applications or scenarios with limited computational resources.\n","\n","**Entropy**\n","\n","**Strengths, Weaknesses, and Use Cases**\n","**Strengths**\n",">Theoretical Soundness: It is more theoretically grounded in information theory.\n","\n",">Sensitivity: More sensitive to class distribution, potentially finding finer, slightly more accurate splits, especially with balanced classes.\n","\n","**Weaknesses:**\n","\n",">Speed: Slower to compute due to the logarithmic operations at every node split.\n","\n",">Can potentially lead to deeper trees, which might increase the risk of overfitting.\n","\n","**Appropriate Use Cases**\n",">Smaller datasets where computational time is less critical.\n","\n",">Situations requiring subtle distinctions between classes or when the goal is to explicitly maximize information gain."],"metadata":{"id":"ozEiMKR4171t"}},{"cell_type":"markdown","source":["**Question 3: What is Pre-Pruning in Decision Trees?**"],"metadata":{"id":"H5WQRdK2cVPc"}},{"cell_type":"markdown","source":["Answer:\n","\n","Pre-Pruning is a technique used to stop the tree from growing too deep by setting limits during training.\n","\n","Common pre-pruning methods:\n","\n","\n","\n",">Maximum depth (max_depth)\n","\n",">Minimum samples split (min_samples_split)\n","\n",">Minimum samples leaf >(min_samples_leaf)\n","\n","Benefit:\n","Reduces overfitting, improves generalization, and speeds up training."],"metadata":{"id":"I-AXnrGYiy22"}},{"cell_type":"markdown","source":["**Question 4**: Write a Python program to train a Decision Tree Classifier using Gini\n","Impurity as the criterion and print the feature importances (practical).\n","\n","Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n","\n","(Include your Python code and output in the code box below.)"],"metadata":{"id":"y-ficKpRjh5C"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# 1. Load a sample dataset (Iris)\n","iris = load_iris()\n","X = pd.DataFrame(iris.data, columns=iris.feature_names)\n","y = iris.target\n","\n","# 2. Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# 3. Train Decision Tree Classifier with Gini impurity\n","clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n","clf.fit(X_train, y_train)\n","\n","# 4. Get feature importances\n","importances = clf.feature_importances_\n","\n","# 5. Print feature importance values\n","print(\"Feature Importances (Gini-based):\")\n","for feature, importance in zip(X.columns, importances):\n","    print(f\"{feature}: {importance:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LaY10D3S0nbt","executionInfo":{"status":"ok","timestamp":1768131035868,"user_tz":-330,"elapsed":23,"user":{"displayName":"Vijay Kumar","userId":"14165771673652212315"}},"outputId":"63072a36-9079-49c7-a4a7-1d18f2c0a80f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature Importances (Gini-based):\n","sepal length (cm): 0.0000\n","sepal width (cm): 0.0191\n","petal length (cm): 0.8933\n","petal width (cm): 0.0876\n"]}]},{"cell_type":"markdown","source":["**Question 5: What is a Support Vector Machine (SVM)?**"],"metadata":{"id":"8xgI7_lpdJoC"}},{"cell_type":"markdown","source":["Answer:\n","\n","Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression. It finds the optimal hyperplane that maximizes the margin between different classes.\n","\n","*Uses support vectors (critical data points)\n","\n","*Effective in high-dimensional spaces"],"metadata":{"id":"fjIVoRWIdetA"}},{"cell_type":"markdown","source":["**Question 6: What is the Kernel Trick in SVM?**"],"metadata":{"id":"Qk-e-EMGd8Ay"}},{"cell_type":"markdown","source":["Answer:\n","\n","The Kernel Trick allows SVM to solve non-linear problems by mapping data into a higher-dimensional space without explicitly computing the transformation.\n","\n","**Common kernels:**\n","\n","* Linear\n","* Polynomial\n","* RBF (Gaussian)\n","* Sigmoid"],"metadata":{"id":"LKn52edleI-a"}},{"cell_type":"markdown","source":["**Question 7**:Write a Python program to train two SVM classifiers with Linear and RBF\n","kernels on the Wine dataset, then compare their accuracies.\n","\n","Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n","on the same dataset.\n","\n","(Include your Python code and output in the code box below.)\n"],"metadata":{"id":"vn0p2YtPeiWe"}},{"cell_type":"code","source":["from sklearn.datasets import load_wine\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# 1. Load the Wine dataset\n","wine = load_wine()\n","X, y = wine.data, wine.target\n","\n","# 2. Split into training and testing sets (70% train, 30% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# 3. Standardize the data (crucial for SVM performance)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# 4. Train SVM with Linear Kernel\n","linear_svc = SVC(kernel='linear')\n","linear_svc.fit(X_train_scaled, y_train)\n","linear_pred = linear_svc.predict(X_test_scaled)\n","linear_acc = accuracy_score(y_test, linear_pred)\n","\n","# 5. Train SVM with RBF (Radial Basis Function) Kernel\n","rbf_svc = SVC(kernel='rbf')\n","rbf_svc.fit(X_train_scaled, y_train)\n","rbf_pred = rbf_svc.predict(X_test_scaled)\n","rbf_acc = accuracy_score(y_test, rbf_pred)\n","\n","# 6. Compare results\n","print(f\"Accuracy with Linear Kernel: {linear_acc:.4f}\")\n","print(f\"Accuracy with RBF Kernel:    {rbf_acc:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4zcgd6FUemN0","executionInfo":{"status":"ok","timestamp":1768131376921,"user_tz":-330,"elapsed":45,"user":{"displayName":"Vijay Kumar","userId":"14165771673652212315"}},"outputId":"a49e1afe-929c-4a8c-cc99-ced198700c06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy with Linear Kernel: 0.9815\n","Accuracy with RBF Kernel:    0.9815\n"]}]},{"cell_type":"markdown","source":["**Question 8**: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?"],"metadata":{"id":"qrkN7t2lfGv8"}},{"cell_type":"markdown","source":["Answer:\n","\n","A Naïve Bayes classifier is a probabilistic machine learning algorithm used primarily for classification tasks, such as spam detection and sentiment analysis. It is based on Bayes' Theorem, which calculates the posterior probability of a class given certain evidence (features).\n","\n","**Why is it called \"Naïve\"**\n","\n","The algorithm is considered \"naïve\" because it makes a strong, simplifying assumption that all features are independent of each other given the class label.\n","\n","* The Assumption: It assumes the presence or absence of one feature has no relationship with any other feature.\n","\n","* Example: To classify a fruit as an orange, the algorithm considers its color (orange), shape (round), and size (3.5 inches) independently. Even if these characteristics are related in nature, the model treats them as separate, independent contributors to the final probability.\n","* Why it's \"Naïve\": In real-world data, features are almost always correlated. For instance, in an email, the word \"Free\" is often followed by \"Money,\" yet the model ignores this dependency.\n","\n","**Key Benefits of this \"Naivety\"**\n","\n","Despite its unrealistic core assumption, this simplification provides several practical advantages:\n","\n","* Computational Efficiency: It requires significantly less training data and is much faster to train than more complex models like SVMs or Neural Networks.\n","* Handling High Dimensions: It excels in text classification where there may be thousands of features (words) because it doesn't need to model complex interactions between them.\n","* Scalability: It is highly scalable, requiring only a single parameter for each feature in a learning problem.\n","\n","**Common Types of Naïve Bayes**\n","\n","Different variants are used depending on the nature of the data.\n","* Gaussian: For continuous features that follow a normal distribution (e.g., height or temperature).\n","* Multinomial: For discrete data, typically word frequencies in text classification.\n","* Bernoulli: For binary data (e.g., whether a word is present or absent in a document)"],"metadata":{"id":"s3NJYcyvfuIS"}},{"cell_type":"markdown","source":["**Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n","Bayes, and Bernoulli Naïve Bayes**\n"],"metadata":{"id":"DDbWsFqofweA"}},{"cell_type":"markdown","source":["Answer:\n","1. **Gaussian Naïve Bayes**\n","\n","Best for: Continuous numerical features\n","Typical use cases: Sensor data, real-valued measurements (height, weight, temperature)\n","\n","**How it works**\n","\n","* Assumes each feature follows a Gaussian (normal) distribution within each class.\n","\n","* Models mean and variance for each feature and uses that to calculate likelihoods.\n","\n","**Example intuition**\n","\n","If you’re trying to classify whether a plant is healthy using leaf width and length which are real numbers, Gaussian NB can model how these continuous measurements vary by class\n","\n","2. **Multinomial Naïve Bayes**\n","\n","Best for: Count or frequency features\n","Typical use cases: Text classification with word counts (spam detection, topic labeling)\n","\n","**How it works**\n","\n","* Assumes features come from a multinomial distribution (think lots of discrete counts).\n","\n","* Uses the counts of features (e.g., number of times each word appears) to compute likelihoods.\n","\n","Why it shines in text\n","\n","Text represented as a “bag of words” becomes a vector of word counts. Multinomial NB naturally models that, making it ideal for many NLP tasks.\n","\n"," 3. **Bernoulli Naïve Bayes**\n","\n","Best for: Binary (presence/absence) features\n","Typical use cases: Indicators like “word present or not in document”, feature flags, yes/no attributes\n","\n","**How it works**\n","\n","* Assumes each feature is Bernoulli-distributed, i.e. takes values 0 or 1.\n","\n","* Models both the presence and absence of features rather than counts.\n","\n","When this helps\n","\n","If you only care whether a word appears at all (not how many times), Bernoulli NB captures that pattern\n","\n","\n","\n","| Variant        | Feature Type           | Distribution Assumed | Typical Use Case             |                      |\n","| -------------- | ---------------------- | -------------------- | ---------------------------- | -------------------- |\n","| Gaussian NB    | Continuous real-valued | Normal (Gaussian)    | Numeric measurements         |                      |\n","| Multinomial NB | Discrete counts        | Multinomial          | Text with word frequencies   |                      |\n","| Bernoulli NB   | Binary flags           | Bernoulli            | Binary presence/absence data | ([GeeksforGeeks][1]) |\n","\n","[1]: https://www.geeksforgeeks.org/bernoulli-naive-bayes/?utm_source=chatgpt.com \"Bernoulli Naive Bayes - GeeksforGeeks\"\n"],"metadata":{"id":"6-NaT3YiggSc"}},{"cell_type":"markdown","source":["**Question 10: Breast Cancer Dataset**\n","\n","Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n","dataset and evaluate accuracy.\n","\n","Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n","sklearn.datasets.\n","(Include your Python code and output in the code box below.)\n"],"metadata":{"id":"-A3Y50aEgl4R"}},{"cell_type":"code","source":["\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","X, y = load_breast_cancer(return_X_y=True)\n","\n","# Split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","\n","# Train model\n","model = GaussianNB()\n","model.fit(X_train, y_train)\n","\n","# Predict and evaluate\n","y_pred = model.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(\"Accuracy:\", accuracy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QKmsAex0gioY","executionInfo":{"status":"ok","timestamp":1768133875234,"user_tz":-330,"elapsed":47,"user":{"displayName":"Vijay Kumar","userId":"14165771673652212315"}},"outputId":"5f5d3baa-d964-408f-83ed-ab8cfe89ee69"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.956140350877193\n"]}]}]}